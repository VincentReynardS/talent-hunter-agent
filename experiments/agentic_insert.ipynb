{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4763dedf",
   "metadata": {},
   "source": [
    "# Agentic Vector Store Interaction\n",
    "\n",
    "This notebook contains experiments regarding making an agent interact with tools that execute operations on a vector store (QDrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "645fd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c8923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c8c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy tools\n",
    "@tool\n",
    "def insert_candidate(name: str, email: str, resume: str):\n",
    "    \"\"\"Use this tool to insert a candidate into the vector database.\"\"\"\n",
    "    return f\"Candidate {name} with email {email} and resume {resume} inserted successfully.\"\n",
    "\n",
    "@tool\n",
    "def search_candidate(query: str):\n",
    "    \"\"\"Use this tool to search for a candidate in the vector database according to the query.\"\"\"\n",
    "    return f\"Candidate that satisfies the query '{query}' searched successfully.\"\n",
    "\n",
    "@tool\n",
    "def insert_company(name: str, company_name: str, job_title: str, company_query: str):\n",
    "    \"\"\"Use this tool to insert a company along with its query and wanted job title into the vector database.\"\"\"\n",
    "    return f\"Company {company_name} that wants a {job_title} and with the query {company_query} inserted successfully.\"\n",
    "\n",
    "@tool\n",
    "def search_company(query: str):\n",
    "    \"\"\"Use this tool to search for a company in the vector database according to the query from the job seeker.\"\"\"\n",
    "    return f\"Company that satisfies the query '{query}' searched successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf520e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = init_chat_model(\n",
    "    \"gpt-4o\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "tools = [insert_candidate, search_candidate, insert_company, search_company]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Bind tools to the LLM\n",
    "model_with_tool = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f4d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful talent hunter that helps job seekers find their dream jobs.\n",
    "You are given a job seeker's query and you need to find the best job for them.\n",
    "Additionally, you also help companies find the best candidates for their jobs.\n",
    "\n",
    "You have access to the following tools:\n",
    "- insert_candidate: to insert a candidate into the vector database\n",
    "- search_candidate: to search for a candidate in the vector database\n",
    "- insert_company: to insert a company into the vector database\n",
    "- search_company: to search for a company in the vector database\n",
    "\n",
    "You need to use the tools to find the best job for the job seeker.\n",
    "You need to use the tools to find the best candidates for the company.\n",
    "\n",
    "The tools may require some information to be inserted into the vector database.\n",
    "You need to ask the job seeker for the information if the tools require it.\n",
    "\n",
    "Sometimes, the job that a job seeker is looking for is not available in the vector database.\n",
    "In this case, just inform the job seeker that the job is not available and ask them to come back later.\n",
    "\n",
    "Likewise, if a suitable candidate is not available for a company, just inform the company that the candidate is not available and ask them to come back later.\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create a model with tools\n",
    "llm_chain = (prompt_template | model_with_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331e2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to call the model\n",
    "def call_model(state: MessagesState, config: RunnableConfig):\n",
    "    response = llm_chain.invoke({\"input\": state[\"messages\"]}, config=config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Create memory saver(memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create a graph\n",
    "agent_graph = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Add graph nodes\n",
    "agent_graph.add_node(\"model\", call_model)\n",
    "agent_graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "agent_graph.add_edge(START, \"model\")\n",
    "agent_graph.add_conditional_edges(\"model\", tools_condition, [\"tools\", END])\n",
    "agent_graph.add_edge(\"tools\", \"model\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = agent_graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf24165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a session\n",
    "config = RunnableConfig(\n",
    "        configurable={\n",
    "            \"thread_id\": \"thread_1\",\n",
    "            \"user_id\": \"1000\",\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        recursion_limit=25\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcca7e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response:\n",
      "Hi there! I'd be happy to help you find a job. Could you please provide me with some more details about the type of job you're looking for, your skills, and any specific industries or companies you're interested in? Additionally, if you have a resume, that would be helpful too.\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(content=\"Hello! I am a job seeker looking for a job.\")\n",
    "\n",
    "result = graph.invoke({\"messages\": [human_message]}, config)\n",
    "\n",
    "print(\"Agent response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad77f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response:\n",
      "Thank you for the information, John! To help you find the best job opportunities, could you please provide your email and a copy of your resume? This will allow me to search for suitable positions and also insert your profile into our database for future opportunities.\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"\n",
    "My name is John Doe. I am a software engineer with 5 years of experience in the industry. I am looking for a job in the field of software engineering.\n",
    "\"\"\"\n",
    "human_message = HumanMessage(content=message)\n",
    "\n",
    "result = graph.invoke({\"messages\": [human_message]}, config)\n",
    "\n",
    "print(\"Agent response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c20904e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m state_snapshot = graph.get_state(config)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pprint.pprint(\u001b[43mstate_snapshot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, width=\u001b[32m100\u001b[39m, depth=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'messages'"
     ]
    }
   ],
   "source": [
    "state_snapshot = graph.get_state(config)\n",
    "\n",
    "pprint.pprint(state_snapshot.values['messages'], width=100, depth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
